{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ad0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eeba870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Amharic poem dataset\n",
    "dataset_path = \"../data/normalized_poems.txt\"\n",
    "\n",
    "# Initialize a BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Use Whitespace pre-tokenization (splits on spaces, preserving Amharic characters)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Define special tokens (same as GPT-2 for compatibility)\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "\n",
    "# Configure the trainer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=12000,  # Adjust based on your dataset size; GPT-2 uses ~50k\n",
    "    min_frequency=2,   # Ignore tokens appearing less than twice\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "# Train the tokenizer on your dataset\n",
    "def get_training_corpus():\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Yield chunks of text to save memory\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"amharic_bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b298608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [435, 146, 5540, 328, 6512, 2496, 1010]\n",
      "Decoded: አንቺ የ ፅዮን ልጅ ዘምሪ እልል በይ\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast\n",
    "\n",
    "# Load the trained tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"amharic_bpe_tokenizer.json\",\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    unk_token=\"<|unk|>\",\n",
    "    pad_token=\"<|pad|>\"\n",
    ")\n",
    "\n",
    "# Save it in a format compatible with Hugging Face\n",
    "tokenizer.save_pretrained(\"amharic_gpt2_tokenizer\")\n",
    "\n",
    "# Test the tokenizer\n",
    "text = \"አንቺ የፅዮን ልጅ ዘምሪ እልል በይ\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded:\", tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610e32b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4412456dec4449d48a7c168dad6f048c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c157a116506c415dba0d878a8ff9e761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdc036468d709f17434d/63bed80836ee0758c8fd4f8975d59bb0b864263ee2753547c358e8a37cde8758?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250514%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250514T172205Z&X-Amz-Expires=3600&X-Amz-Signature=ba7a0285fb4841ab02be3e3db0e8213e9dcee64afbdedfb3b7b5b1a1e30868d9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&x-id=GetObject&Expires=1747246925&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzI0NjkyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMDM2NDY4ZDcwOWYxNzQzNGQvNjNiZWQ4MDgzNmVlMDc1OGM4ZmQ0Zjg5NzVkNTliYjBiODY0MjYzZWUyNzUzNTQ3YzM1OGU4YTM3Y2RlODc1OCoifV19&Signature=H0YyRX7PBtidqpFrZqR2zg-hOeAcD09aheF2hnPI6xagHJHy2-W48JlBZerPd-Hs8tRMhogFWFiNKF8TvXGehVKZ91sU-eVuAmDn1D0IJJyh5PrFmD-u90i-bq79W03CdzU5pqomNyW4w28EBUFgmdgg%7ETPni-j5sgUWUbwkdbsPwC1d%7EpYBNeehCJBYCo1tzCIhx2vIv%7Eer-9VavaW15pfJGo6l3ZD62A6UvFeLIPQkwSnyrAq1Hh-LsfX9D216hdAvU6dFmPxvmqVZ50c%7ER0vmFGjv3J767PjmhYpDynFMH5iktLtOupd19hQ2BOHZupxMh-eHl3ntgL98rMtD9A__&Key-Pair-Id=K2L8F4GPSG1IFC: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb97258a20f42c4bace03c1c5fa29a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d491faf4ab94bff9e86072c839af04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(7212, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Resize the token embeddings to match the new tokenizer's vocabulary\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5d40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
